# Lab 02: NLP Preprocessing Techniques

This lab introduced the essential steps in cleaning and preparing raw text data for NLP tasks. I worked with both **NLTK** and **spaCy** to understand the differences between their tools and how preprocessing choices affect downstream results.

---

### ğŸ”§ Techniques Used
- Tokenization (word and sentence)
- Stop word removal
- Lemmatization vs. Stemming
- Text cleaning and normalization
- Building a full preprocessing pipeline

---

### ğŸ§  Key Takeaways
- **Cleaning text** is like preparing a messy room before inviting someone in â€” machines canâ€™t understand raw language like humans do.
- Different libraries (like NLTK and spaCy) process text differently â€” even stop word lists vary, and those choices matter.
- **Lemmatization** is better when accuracy and understanding are key (e.g., in a chatbot), while **stemming** is faster but rougher.
- Emojis, hashtags, and punctuation can carry important meaning in real-world data (like social media sentiment).
- There's no one-size-fits-all method â€” preprocessing depends on the **goal of your NLP system**.

---

### ğŸ’¬ Personal Reflection
This lab helped me understand that preprocessing isnâ€™t just a simple setup step â€” itâ€™s a critical part of making NLP work well. I realized that small choices, like whether or not to remove a word like *â€œnotâ€*, can totally change the meaning and impact the results. I also started thinking about how this applies to other languages, platforms, and real-world applications like chatbots or review analysis.

---

### ğŸ—‚ Files Included
- `L02_Gregory_Livingston_ITAI_2373.ipynb` â€” Lab notebook
- `L02_Journal_Gregory_Livingston_ITAI_2373.pdf` â€” Reflection journal

---

**Course:** ITAI 2373 â€“ Natural Language Processing   
**Submitted:** June 10, 2025
