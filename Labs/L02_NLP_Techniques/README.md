# Lab 02: NLP Preprocessing Techniques

This lab introduced the essential steps in cleaning and preparing raw text data for NLP tasks. I worked with both **NLTK** and **spaCy** to understand the differences between their tools and how preprocessing choices affect downstream results.

---

### 🔧 Techniques Used
- Tokenization (word and sentence)
- Stop word removal
- Lemmatization vs. Stemming
- Text cleaning and normalization
- Building a full preprocessing pipeline

---

### 🧠 Key Takeaways
- **Cleaning text** is like preparing a messy room before inviting someone in — machines can’t understand raw language like humans do.
- Different libraries (like NLTK and spaCy) process text differently — even stop word lists vary, and those choices matter.
- **Lemmatization** is better when accuracy and understanding are key (e.g., in a chatbot), while **stemming** is faster but rougher.
- Emojis, hashtags, and punctuation can carry important meaning in real-world data (like social media sentiment).
- There's no one-size-fits-all method — preprocessing depends on the **goal of your NLP system**.

---

### 💬 Personal Reflection
This lab helped me understand that preprocessing isn’t just a simple setup step — it’s a critical part of making NLP work well. I realized that small choices, like whether or not to remove a word like *“not”*, can totally change the meaning and impact the results. I also started thinking about how this applies to other languages, platforms, and real-world applications like chatbots or review analysis.

---

### 🗂 Files Included
- `L02_Gregory_Livingston_ITAI_2373.ipynb` — Lab notebook
- `L02_Journal_Gregory_Livingston_ITAI_2373.pdf` — Reflection journal

---

**Course:** ITAI 2373 – Natural Language Processing   
**Submitted:** June 10, 2025
